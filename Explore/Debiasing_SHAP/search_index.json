[["index.html", "Untrustworthy explainable AI for Trees Chapter 1 Preface", " Untrustworthy explainable AI for Trees M Loecher and Marvin Wright 2021-01-17 Chapter 1 Preface "],["intro.html", "Chapter 2 Introduction 2.1 Titanic 2.2 Titanic Regression Trees 2.3 Titanic Classification Trees 2.4 Simulated Data", " Chapter 2 Introduction We unify the various recent attempts to (i) improve the interpretability of tree-based models and (ii) debias the the default variable-importance measure (MDI) in random forests. In particular, we demonstrate a common thread among the out-of-bag based bias correction methods and their connection to local explanation for trees. In addition, we point out a bias caused by the inclusion of inbag data in the newly developed SHAP values. Empirical and simulational studies indicate substantial improvements in the discriminative power of SHAP values when out- of-sample data are used instead. 2.1 Titanic We will begin by illustrating the basic problem on the Titanic data. 2.2 Titanic Regression Trees rf = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100,max_features=2) tmp=rf.fit(X, Y) explainer = shap.TreeExplainer(rf) shap_values = explainer.shap_values(X) shap.summary_plot(shap_values, X) shap.summary_plot(shap_values, X, plot_type=&quot;bar&quot;) 2.3 Titanic Classification Trees rf2 = RandomForestClassifier(max_depth=50, random_state=0, n_estimators=100,max_features=2) tmp=rf2.fit(X, Y) explainer2 = shap.TreeExplainer(rf2) shap_values2 = explainer2.shap_values(X) shap.summary_plot(shap_values2, X, plot_type=&quot;bar&quot;) 2.4 Simulated Data We replicate the simulation design used by where a binary response variable Y is predicted from a set of \\(5\\) predictor variables that vary in their scale of measurement and number of categories. The first predictor variable \\(X_1\\) is continuous, while the other predictor variables \\(X_2 ,\\ldots, X_5\\) are multinomial with \\(2, 4, 10, 20\\) categories, respectively. The sample size for all simulation studies was set to n = 120. In the first all predictor variables and the response are sampled independently. We would hope that a reasonable variable importance measure would not prefer any one predictor variable over any other. In the second simulation study, the so-called , the distribution of the response is a binomial process with probabilities that depend on the value of \\(x_2\\), namely \\(P(y=1|X_2=1)=0.35, P(y=1|X_2=2)=0.65\\) . As is evident in the two leftmost panels of Figure 2.1, both the Gini importance (MDI) and the SHAP values show a strong preference for variables with many categories and the continuous variable. This bias is of course well-known for MDI but maybe unexpected for the SHAP scores. Figure 2.1: Results of the null case, where none of the predictor variables is informative. The results from the power study are summarized in Figure 2.2. MDI and SHAP again show a strong bias towards variables with many categories and the continuous variable. At the chosen sigal-to-noise ratio MDI fails entirely to identify the relevant predictor variable. In fact, the mean value for the relevant variable \\(X_2\\) is lowest and only slightly higher than in the null case. Figure 2.2: Results of the power study, where only \\(X_2\\) is informative. "],["Supervised-SHAP.html", "Chapter 3 Supervised SHAP 3.1 Motivation 3.2 in-outbag correlations 3.3 Debias Proposals", " Chapter 3 Supervised SHAP The signal-to-noise separation for the SHAP values is moderate but can be greatly improved by mutliplying with \\(y_i\\) before averaging, as shown in Figure 3.1. Figure 3.1: Weighted SHAP values, as explained in the text. Left graph: power study, where only \\(X_2\\) is informative. Right graph: null case, where none of the predictor variables is informative. Other simulation details as before. 3.1 Motivation Why multiply by Y ? I believe that ultimately the misleading feature importances are an overfitting problem. The global importance scores are averages of the absolute values of the SHAP values, so they reflect merely variation; regardless whether that variation reflects the truth at all. So for e.g. passengerID the model will still produce widely varying SHAP values even on a testset (or oob) -leading to inflated importance - but we would want to penalize the wrong direction! (Not possible on the training data as the model was fit in order to optimize the agreement with \\(Y_{train}\\)). Figure 3.2 shows the results for the Titanic data: Figure 3.2: Left graph: raw SHAP values for the Titanic data, separately computed for inbag and OOB. Right graph: weighted SHAP values are multiplied by \\(y_i\\) before averaging which eliminates the spurious contributions due to extit{passengerID} for OOB. Note that we scaled the SHAP values to their respective maxima for easier comparison. For the inbag data we observe a strong correlation between the sign of the SHAP vales and the sign of \\(Y_{train}\\), whereas this gap disappears almost entirely for those features that suffer from overfitting (such as passenger ID): Figure 3.3: The difference in SHAP distributions the sign of \\(Y_{train}\\) is greatest for informative features. 3.2 in-outbag correlations For new data, multiplying by Y will not be feasible anyhow, so we need to look for other ways of correcting the SHAP values. Figure 3.4 suggests that the correlation between inbag and oob SHAP values is an indicator for the degree of overfitting. MSE or \\(R^2\\) (or some other measure of goodness of fit) could be used as an indicator for the reliability of the importance measures. Figure 3.4: The correlation between inbag and oob SHAP values is greatest for informative features. 3.3 Debias Proposals For each feature Fit a (linear?) model \\(SHAP_{i, oob} = \\beta_1 \\cdot SHAP_{i, inbag} + u_i\\) Use the estimates \\(\\widehat{SHAP}_{i, oob} = \\hat{\\beta_1} \\cdot SHAP_{i, inbag}\\) as local explanations instead of either \\(SHAP_{i, oob}\\) or \\(SHAP_{i, inbag}\\), We can think of the predictions as a regularized or smoothed version of the original SHAP values. The shrinkage depends on the correlation between inbag and oob: Shrunk SHAP for Titanic For the simple case of the Titanic data, the smoothing of the SHAP values looks promising. Shrunk SHAP for Simulated Data However, for the simulated data much less so. NULL simulation Power simulation I feel that the signal-to-noise ratio of the simulated data is low we might want to tune the model first "],["Honest-Trees.html", "Chapter 4 Honest Trees", " Chapter 4 Honest Trees Inspired by honest trees: we want to recompute SHAP values on trees where the node predictions (based on the inbag data) have been replaced by oob estimates. (Problem: the oob data are much smaller and lead to many empty nodes, so we have to prune the trees!) In analogy to Figure 3.3 we use boxplots facilitate easy comparison between inbag and oob as well as honest tree versions (ob2) SHAP values Figure 4.1: The difference in SHAP distributions the sign of \\(Y_{train}\\) is greatest for informative features. In analogy to Figure 3.4 par(mfrow=c(2,2)) for (j in 1:4){ fit = lm(shap_oob_avg[,j] ~ shap_inbag_avg[,j]) R2 = round(summary(fit)$r.sq,2) plot(shap_inbag_avg[,j],shap_oob_avg[,j],col=rgb(0,0,1,0.5), pch=20,cex=0.75, xlab = &quot;Inbag SHAP&quot;, ylab=&quot;oob shap&quot;, main = paste0(colnames(shap_inbag_avg)[j], &quot; (R2 =&quot;, R2,&quot;)&quot;));grid() abline(fit,col=2) } par(mfrow=c(2,2)) for (j in 1:4){ fit = lm(shap_oob2_avg[,j] ~ shap_inbag_avg[,j]) R2 = round(summary(fit)$r.sq,2) plot(shap_inbag_avg[,j],shap_oob2_avg[,j],col=rgb(0,0,1,0.5), pch=20,cex=0.75, xlab = &quot;Inbag SHAP&quot;, ylab=&quot;oob2 shap&quot;, main = paste0(colnames(shap_inbag_avg)[j], &quot; (R2 =&quot;, R2,&quot;)&quot;));grid() abline(fit,col=2) } "],["dof.html", "Chapter 5 Degrees of Freedom", " Chapter 5 Degrees of Freedom Id like to quote this zinger from the book Numerical Recipes: if the difference between n and n1 ever matters to you, then you are probably up to no good anyway - e.g., trying to substantiate a questionable hypothesis with marginal data. "],["Overfitting.html", "Chapter 6 Overfitting 6.1 Tree Depth 6.2 Signal to Noise", " Chapter 6 Overfitting I am often asked whether we should blame the model or the importance measure/ feature contribution for the apparent overfitting which leads to bias. Maybe at least some of our results are due to a lack of model tuning ? library(titanic) library(ranger) naRows = is.na(titanic_train$Age) data2=titanic_train[!naRows,] rf1 =ranger(Survived ~ Age + Sex + Pclass + PassengerId, data=data2, num.trees=100,mtry=2) rf2 =ranger(Survived ~ Age + Sex + Pclass , data=data2, num.trees=100,mtry=2) oob1 = round(rf1$prediction.error,3) oob2 = round(rf2$prediction.error,3) It is certainly true that e.g. for the Titanic data the random forest model performs better without the PassengerId variable than with (oob prediction error of 0.133 versus 0.14). This observation is only partially useful since feature selection is a complex problem and clearly not guided by SHAP or MDI values. 6.1 Tree Depth SHAP as a function of tree depth # Power Study simulations run earlier load(&quot;data/shap_avs.rda&quot;) In their paper the paper A Debiased MDI Feature Importance Measure for Random Forests (Li et al. 2019) investigate the dependence of MDI on minimum leaf size and tree depth. To mimic the major experiment setting in the paper (Strobl et al. 2007), data is generated as follows. We sample n = 200 observations, each containing 5 features. The first feature is generated from standard Gaussian distribution. The second feature is generated from a Bernoulli distribution with \\(p = 0.5\\). The third/fourth/fifth features have 4/10/20 categories respectively with equal probability of taking any states. The response label y is generated from a Bernoulli distribution such that \\(P(y_i = 1) = (1 + x_{i2})/3\\). We vary the max tree depth of RF from 1 to 20 and record the MDI of every feature. The results are shown in Fig. 2 below. As tree depth increases, the minimum leaf size generally decreases exponentially. Therefore, we expect the MDI of noisy features to become larger for increasing tree depth. We vary the maximum depth from 1 to 20 and record the MDI of every feature. The results shown in Fig. 2 are consistent with our expectation. MDI importance of noisy features increase when the tree depth increases from 1 to 20. We computed inbag and oob SHAP values for the same data and display the results in Figure 6.1. par(mfrow=c(1,2)) matplot(shap_depth_in , type=&quot;l&quot;,lty=1,lwd=2,xlab = &quot;tree depth&quot;, ylab = &quot;SHAP Importance&quot;,xlim = c(1,15), main = &quot;inbag&quot;);grid() #legend(&quot;topright&quot;, lty=1,lwd=2, col=1:5, legend = colnames(shap_depth_oob)) matplot(shap_depth_oob , type=&quot;l&quot;,lty=1,lwd=2,xlab = &quot;tree depth&quot;, ylab = &quot;SHAP Importance&quot;,xlim = c(1,19), main = &quot;OOB&quot;);grid() legend(&quot;topright&quot;, lty=1,lwd=2, col=1:5, legend = colnames(shap_depth_oob)) Figure 6.1: SHAP values as a function of tree depth. 6.2 Signal to Noise References "],["references.html", "References", " References "],["Appendix.html", "Chapter 7 Appendix 7.1 Honest Trees Code", " Chapter 7 Appendix bla bla 7.1 Honest Trees Code "]]

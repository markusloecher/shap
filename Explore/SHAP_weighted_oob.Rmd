---
title: "SHAP values for inbag vs. oob data"
author: "M Loecher"
date: "25 11 2020"
output: 
  html_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(ggplot2)
#py_install("shap")
rerun = FALSE #rerun python code to compute SHAP values ?
#If set to FALSE, the file "shap_inbag_oob.rda" is read in instead
if (!rerun) load("shap_inbag_oob.rda")
```


```{python}
import pandas as pd
import numpy as np
np.random.seed(0)
import matplotlib.pyplot as plt
import shap
import pickle

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor

rerun = r.rerun
```


## Setting up the data 

```{python, eval = rerun}
df = pd.read_csv('https://raw.githubusercontent.com/markusloecher/shap/master/Explore/titanicnoMissingAge.csv') # Load the data
#df
Y = df['Survived']
X =  df[['Age', 'Pclass','Sex', 'PassengerId']]
# Split the data into train and test data:
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)
```

## Defining our python functions

```{python, eval = rerun}
def plotImp(importances,features, main = 'Feature Importances',xlab='Relative Importance'):
    indices = np.argsort(importances)
    #features = X_train.columns
    plt.title(main)
    plt.barh(range(len(indices)), importances[indices], color='b', align='center')
    plt.yticks(range(len(indices)), [features[i] for i in indices])
    plt.xlabel(xlab)
    plt.show()
    
def oob_regression_r2_score(rf, X_train, y_train):
    """
    Compute out-of-bag (OOB) R^2 for a scikit-learn random forest
    regressor. We learned the guts of scikit's RF from the BSD licensed
    code:
    https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L702
    """
    X = X_train.values if isinstance(X_train, pd.DataFrame) else X_train
    y = y_train.values if isinstance(y_train, pd.Series) else y_train

    n_samples = len(X)
    predictions = np.zeros(n_samples)
    n_predictions = np.zeros(n_samples)
    for tree in rf.estimators_:
        unsampled_indices = generate_unsampled_indices(tree.random_state, n_samples)
        tree_preds = tree.predict(X[unsampled_indices, :])
        predictions[unsampled_indices] += tree_preds
        n_predictions[unsampled_indices] += 1

    if (n_predictions == 0).any():
        warnings.warn("Too few trees; some variables do not have OOB scores.")
        n_predictions[n_predictions == 0] = 1

    predictions /= n_predictions

    oob_score = r2_score(y, predictions)
    return oob_score

#http://bakfu.github.io/doc/_modules/sklearn/ensemble/forest.html
from sklearn.utils import check_random_state #, check_array, compute_sample_weight
#from sklearn.utils.fixes import bincount

def generate_sample_indices(random_state, n_samples):
    """Private function used to _parallel_build_trees function."""
    random_instance = check_random_state(random_state)
    sample_indices = random_instance.randint(0, n_samples, n_samples)

    return sample_indices

def generate_unsampled_indices(random_state, n_samples):
    """Private function used to forest._set_oob_score fuction."""
    sample_indices = generate_sample_indices(random_state, n_samples)
    sample_counts = np.bincount(sample_indices, minlength=n_samples)
    unsampled_mask = sample_counts == 0
    indices_range = np.arange(n_samples)
    unsampled_indices = indices_range[unsampled_mask]

    return unsampled_indices
```


## Fitting a random forest

```{python, eval = rerun}
rf = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100,max_features=2)
rf.fit(X_train, Y_train)  
print(rf.feature_importances_)
importances = rf.feature_importances_
features = X_train.columns

plotImp(importances,features)
```

## Computing shap separately for inbag and oob:

```{python, eval = rerun}
n_samples, p = X_train.shape

if (rerun):
    explainer = shap.TreeExplainer(rf)
    shapGlobal = explainer.shap_values(X_train)

    k=0
    shap_oob = np.zeros((n_samples,p, rf.n_estimators))
    shap_inbag = np.zeros((n_samples,p, rf.n_estimators))
    for tree in rf.estimators_:
      tree_preds = tree.predict(X_train)
      unsampled_indices = generate_unsampled_indices(tree.random_state, n_samples)
      sampled_indices = generate_sample_indices(tree.random_state, n_samples)
      explainer = shap.TreeExplainer(tree)
      shap_oob[unsampled_indices,:,k] = explainer.shap_values(X_train.iloc[unsampled_indices,:])
      shap_inbag[sampled_indices,:,k] = explainer.shap_values(X_train.iloc[sampled_indices,:])
      #print(k)
      k+=1
```

```{python, eval = rerun}
shap_oob_avg = np.sum(shap_oob, axis=2) 
shap_inbag_avg = np.sum(shap_inbag, axis=2)

globalSHAPImp_oob =np.sum(np.abs(shap_oob_avg), axis=0)
globalSHAPImp_inbag = np.sum(np.abs(shap_inbag_avg), axis=0)
```


### Copying python objects into R 

```{r, eval = rerun}
shap_inbag_avg =py$shap_inbag_avg 
shap_oob_avg =py$shap_oob_avg 
colnames(shap_inbag_avg) = colnames(shap_oob_avg) = c('Age', 'Pclass','Sex', 'PassengerId')

Y_train = py$Y_train
X_train = py$X_train
X_test = py$X_test
Y_test = py$Y_test
  
save(shap_inbag_avg, shap_oob_avg, X_train, X_test, Y_train, Y_test, file = "shap_inbag_oob.rda")
```

# R code only

```{r}

Y_train2 = Y_train
Y_train2[Y_train==0] = -1
shap_inbag_avg_means = colMeans(abs(shap_inbag_avg))
shap_oob_avg_means = colMeans(abs(shap_oob_avg))
```

## "Raw" SHAP values

```{r, fig.width=10,fig.height=6}
par(mfrow=c(2,2))
boxplot(shap_inbag_avg,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Inbag SHAP");grid()
barplot(shap_inbag_avg_means,col="bisque",main = "Inbag SHAP")
boxplot(shap_oob_avg,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Outbag SHAP");grid()
barplot(shap_oob_avg_means,col="bisque",main = "OOB SHAP")
#means = colMeans(mdiScore2[[4]]);points(means,col="red",pch=18)

```

## Weighting by Y 

```{r}
shap_inbag_wght = shap_inbag_avg
shap_oob_wght = shap_oob_avg

for (i in 1:4) {
  shap_inbag_wght[,i] = Y_train*shap_inbag_avg[,i]
  shap_oob_wght[,i] = Y_train*shap_oob_avg[,i]
}

shap_inbag_wght_means = colMeans(abs(shap_inbag_wght))
shap_oob_wght_means = colMeans(abs(shap_oob_wght))
```

```{r, fig.width=10,fig.height=6}
par(mfrow=c(2,2))
boxplot(shap_inbag_wght,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Inbag SHAP");grid()
barplot(shap_inbag_wght_means,col="bisque",main = "Inbag SHAP")
boxplot(shap_oob_wght,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Outbag SHAP");grid()
barplot(shap_oob_wght_means,col="bisque",main = "OOB SHAP")

```

## Weighting by Y2 

Whether we code the output as $(0,1)$ or as $(-1, 1)$ makes no difference:

```{r}
shap_inbag_wght = shap_inbag_avg
shap_oob_wght = shap_oob_avg

for (i in 1:4) {
  shap_inbag_wght[,i] = Y_train2*shap_inbag_avg[,i]
  shap_oob_wght[,i] = Y_train2*shap_oob_avg[,i]
}

shap_inbag_wght_means = colMeans(abs(shap_inbag_wght))
shap_oob_wght_means = colMeans(abs(shap_oob_wght))
```

```{r, fig.width=10,fig.height=6}
par(mfrow=c(2,2))
boxplot(shap_inbag_wght,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Inbag SHAP");grid()
barplot(shap_inbag_wght_means,col="bisque",main = "Inbag SHAP")
boxplot(shap_oob_wght,col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "Outbag SHAP");grid()
barplot(shap_oob_wght_means,col="bisque",main = "OOB SHAP")

```

# Motivation

Why multiply by Y ? 

I believe that ultimately the misleading feature importances are an overfitting problem
The global importance scores are averages of the **absolute values** of the SHAP values, so they reflect merely variation; regardless whether that variation reflects the truth at all.
So for e.g. passengerID the model will still produce widely varying SHAP values even on a testset (or oob) -leading to inflated importance - but we would want to "penalize" the wrong direction!
(Not possible on the training data as the model was fit in order to optimize the agreement with $Y_{train}$).


## Inbag versus Outbag

For the inbag data we observe a strong correlation between the sign of the SHAP vales and the sign of $Y_{train}$, whereas this gap disappears almost entirely for those features that suffer from overfitting (such as passenger ID):

```{r, fig.width=10,fig.height=6}
N = nrow(shap_inbag_avg)
shap = list()

#sigh,yes, I know, what a clumsy way to create a long data frame!
for (j in 1:4){
  shap[[j]] = cbind.data.frame(shap=c(shap_inbag_avg[,j], oob = shap_oob_avg[,j]), bag = rep(c("inbag","oob"), each=N), Y_train2=factor(c(Y_train2,Y_train2)))
  #boxplot(shap ~ bag*Y_train2, data = shap, col=rgb(0,0,1,0.5), outcol=rgb(0,0,1,0.5),pch=20,cex=0.75, ylab = "SHAP", main = colnames(shap_inbag_avg)[j]);grid()
}
shap = do.call("rbind.data.frame", shap)
shap$feature = rep(colnames(shap_inbag_avg), each =2*N)
  
ggplot(shap, aes(x=Y_train2, y=shap, fill=bag)) + geom_boxplot() + facet_wrap(~ feature)

```


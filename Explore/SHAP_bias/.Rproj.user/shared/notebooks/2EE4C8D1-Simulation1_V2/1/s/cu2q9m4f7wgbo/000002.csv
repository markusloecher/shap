"0","
def SimulateData_simple(n=120, # number of rows in data
                        M=100, # number of simulations
                        #nCores = M, # number of cores to use; set to 1 on Windows!
                        relevance = 0.15, # signal strength (0 for NULL)
                        inoutbag=False, 
                        ntree = 100, #number of trees in forest
                        #correctBias = c(inbag=TRUE,outbag=TRUE),
                        verbose=0,
                        n_features = 5):
  '''
  # qi: note: added some description here 
  shap_vals_oob and shap_vals_in, shape= (M*n_samples,K) which stores the simulation data

  shap_avs_oob and shap_avs_in , shape = (M*1,K), which sum up n_samples number of value in shap_vals_oob and shap_vals_in
  '''

  random.seed(123)
  shap_avs = np.zeros(n_features) # Initializes the first array
  shap_avs_oob = np.zeros(n_features); shap_avs_in = np.zeros(n_features)
  ft_importances = np.zeros(n_features)# Initializes the first array
  shap_vals = np.zeros(n_features)# Initializes the first array
  shap_vals_oob = np.zeros(n_features); shap_vals_in = np.zeros(n_features)
  allDFs = [] # List of DFs that will be filled 
  y_train = []

  for i in range(M):

    x1 = np.random.randn(n)
    x2 = np.random.randint(1, 3, n)
    x3 = np.random.randint(1, 5, n)
    x4 = np.random.randint(1, 11, n)
    x5 = np.random.randint(1, 21, n)
    # y = np.random.binomial(n = 1, p = 0.5 + [-1,1][x2[0]-1] * relevance, size = n)
    y = np.array([]) 
    for i in range(n):
      y = np.append(y, np.random.binomial(n = 1, p = 0.5 + [-1,1][x2[i]-1] * relevance, size = 1)) # qi: Remark: p need to <=1 and >=0, when relevance > 0.5 this condition doesn not hold anymore

    x_train = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5}, columns=['x1', 'x2', 'x3', 'x4', 'x5'])
    # y_train = pd.DataFrame({'y': y}, columns=['y'])

    rf = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=ntree,max_features=n_features) 
    rf.fit(x_train, y)
    feature_importances = rf.feature_importances_
    # print(feature_importances)
    ft_importances = np.vstack((ft_importances, feature_importances))
    allDFs.append(x_train)
    y_train.append(y)
    
    warnings.filterwarnings('ignore')
    if (inoutbag):
        shap_oob,shap_inbag,shap_oob_avg,shap_inbag_avg,globalSHAPImp_oob,globalSHAPImp_inbag = shap_values_oob(x_train, rf)
        shap_vals_oob = np.vstack((shap_vals_oob, shap_oob_avg))
        shap_vals_in = np.vstack((shap_vals_in, shap_inbag_avg))
        shap_avs_oob = np.vstack((shap_avs_oob, globalSHAPImp_oob))
        shap_avs_in = np.vstack((shap_avs_in, globalSHAPImp_inbag))
        #return(shap_oob,shap_inbag,shap_oob_avg,shap_inbag_avg,globalSHAPImp_oob,globalSHAPImp_inbag, rlvFtrs)
    else:    
        shap_values = shap.TreeExplainer(rf).shap_values(x_train)
        shap_averages = np.mean(np.absolute(shap_values), axis=0) # change to mean

        shap_vals = np.vstack((shap_vals, shap_values))
        shap_avs = np.vstack((shap_avs, shap_averages))
  
  ft_importances = np.delete(ft_importances, (0), axis=0) # Deletes the initialization

  if (inoutbag):
      shap_vals_oob = np.delete(shap_vals_oob, (0), axis=0) # Deletes the initialization
      shap_vals_in = np.delete(shap_vals_in, (0), axis=0)
      shap_avs_oob = np.delete(shap_avs_oob, (0), axis=0) # Deletes the initialization
      shap_avs_in = np.delete(shap_avs_in, (0), axis=0)
      return(shap_vals_oob,shap_vals_in,shap_avs_oob, shap_avs_in, ft_importances, allDFs, n, y_train)
  else: 
      shap_vals = np.delete(shap_vals, (0), axis=0) # Deletes the initialization
      shap_avs = np.delete(shap_avs, (0), axis=0) # Deletes the initialization
      return(shap_vals, shap_avs, ft_importances, allDFs, n)
"

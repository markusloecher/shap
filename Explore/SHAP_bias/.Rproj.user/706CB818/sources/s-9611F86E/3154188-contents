---
title: "Simulation1"
author: "Wu Qi"
date: "27/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate) # qi: note: need to reload python packages into miniconda,e.g. py_install("pandas")
WIDTH = 5
HEIGHT = 5
```

### Import Python Packages
```{python}
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import shap 

import warnings
import time
import datetime

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
```

### Python Code: define function 
```{python}
#https://github.com/parrt/random-forest-importances/blob/master/src/rfpimp.py
def oob_regression_r2_score(rf, X_train, y_train): # qi: note: this function is not used yet
    """
    Compute out-of-bag (OOB) R^2 for a scikit-learn random forest
    regressor. We learned the guts of scikit's RF from the BSD licensed
    code:
    https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/ensemble/forest.py#L702
    """
    from sklearn.metrics import r2_score
    X = X_train.values if isinstance(X_train, pd.DataFrame) else X_train
    y = y_train.values if isinstance(y_train, pd.Series) else y_train

    n_samples = len(X)
    predictions = np.zeros(n_samples)
    n_predictions = np.zeros(n_samples)
    for tree in rf.estimators_:
        unsampled_indices = generate_unsampled_indices(tree.random_state, n_samples)
        tree_preds = tree.predict(X[unsampled_indices, :])
        predictions[unsampled_indices] += tree_preds
        n_predictions[unsampled_indices] += 1

    if (n_predictions == 0).any():
        warnings.warn("Too few trees; some variables do not have OOB scores.")
        n_predictions[n_predictions == 0] = 1

    predictions /= n_predictions

    oob_score = r2_score(y, predictions)
    return oob_score

#http://bakfu.github.io/doc/_modules/sklearn/ensemble/forest.html
from sklearn.utils import check_random_state #, check_array, compute_sample_weight
#from sklearn.utils.fixes import bincount

def generate_sample_indices(random_state, n_samples):
    """Private function used to _parallel_build_trees function."""
    random_instance = check_random_state(random_state)
    sample_indices = random_instance.randint(0, n_samples, n_samples)

    return sample_indices

def generate_unsampled_indices(random_state, n_samples):
    """Private function used to forest._set_oob_score fuction."""
    sample_indices = generate_sample_indices(random_state, n_samples)
    sample_counts = np.bincount(sample_indices, minlength=n_samples)
    unsampled_mask = sample_counts == 0
    indices_range = np.arange(n_samples)
    unsampled_indices = indices_range[unsampled_mask]

    return unsampled_indices

def shap_values_oob(X_train, rf):
    n_samples, p = X_train.shape
    shap_oob = np.zeros((n_samples, p, rf.n_estimators))
    shap_inbag = np.zeros((n_samples, p, rf.n_estimators))
    for k,tree in enumerate(rf.estimators_):
      tree_preds = tree.predict(X_train)
      unsampled_indices = generate_unsampled_indices(tree.random_state, n_samples)
      sampled_indices = generate_sample_indices(tree.random_state, n_samples)
      explainer = shap.TreeExplainer(tree)
      shap_oob[unsampled_indices,:,k] = explainer.shap_values(X_train.iloc[unsampled_indices,:])
      shap_inbag[sampled_indices,:,k] = explainer.shap_values(X_train.iloc[sampled_indices,:])
    
    shap_oob_avg = np.mean(shap_oob, axis=2) # average the value for rf.n_estimators numbers of trees
    shap_inbag_avg = np.mean(shap_inbag, axis=2)
    globalSHAPImp_oob =np.mean(np.abs(shap_oob_avg), axis=0)# average the shap value for every observation
    globalSHAPImp_inbag = np.mean(np.abs(shap_inbag_avg), axis=0)
    return shap_oob,shap_inbag,shap_oob_avg,shap_inbag_avg,globalSHAPImp_oob,globalSHAPImp_inbag
```

### Define function to simulate data
```{python}

def SimulateData_simple(n=120, # number of rows in data
                        M=100, # number of simulations
                        #nCores = M, # number of cores to use; set to 1 on Windows!
                        relevance = 0.15, # signal strength (0 for NULL)
                        inoutbag=False, 
                        ntree = 100, #number of trees in forest
                        #correctBias = c(inbag=TRUE,outbag=TRUE),
                        verbose=0,
                        n_features = 5):
  '''
  # qi: note: added some description here 
  shap_vals_oob and shap_vals_in, shape= (M*n_samples,K) which stores the simulation data

  shap_avs_oob and shap_avs_in , shape = (M*1,K), which sum up n_samples number of value in shap_vals_oob and shap_vals_in
  '''

  random.seed(123)
  shap_avs = np.zeros(n_features) # Initializes the first array
  shap_avs_oob = np.zeros(n_features); shap_avs_in = np.zeros(n_features)
  ft_importances = np.zeros(n_features)# Initializes the first array
  shap_vals = np.zeros(n_features)# Initializes the first array
  shap_vals_oob = np.zeros(n_features); shap_vals_in = np.zeros(n_features)
  allDFs = [] # List of DFs that will be filled 
  y_train = []

  for i in range(M):

    x1 = np.random.randn(n)
    x2 = np.random.randint(1, 3, n)
    x3 = np.random.randint(1, 5, n)
    x4 = np.random.randint(1, 11, n)
    x5 = np.random.randint(1, 21, n)
    # y = np.random.binomial(n = 1, p = 0.5 + [-1,1][x2[0]-1] * relevance, size = n)
    y = np.array([]) 
    for i in range(n):
      y = np.append(y, np.random.binomial(n = 1, p = 0.5 + [-1,1][x2[i]-1] * relevance, size = 1)) # qi: Remark: p need to <=1 and >=0, when relevance > 0.5 this condition doesn not hold anymore

    x_train = pd.DataFrame({'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4, 'x5': x5}, columns=['x1', 'x2', 'x3', 'x4', 'x5'])
    # y_train = pd.DataFrame({'y': y}, columns=['y'])

    rf = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=ntree,max_features=n_features) 
    rf.fit(x_train, y)
    feature_importances = rf.feature_importances_
    # print(feature_importances)
    ft_importances = np.vstack((ft_importances, feature_importances))
    allDFs.append(x_train)
    y_train.append(y)
    
    warnings.filterwarnings('ignore')
    if (inoutbag):
        shap_oob,shap_inbag,shap_oob_avg,shap_inbag_avg,globalSHAPImp_oob,globalSHAPImp_inbag = shap_values_oob(x_train, rf)
        shap_vals_oob = np.vstack((shap_vals_oob, shap_oob_avg))
        shap_vals_in = np.vstack((shap_vals_in, shap_inbag_avg))
        shap_avs_oob = np.vstack((shap_avs_oob, globalSHAPImp_oob))
        shap_avs_in = np.vstack((shap_avs_in, globalSHAPImp_inbag))
        #return(shap_oob,shap_inbag,shap_oob_avg,shap_inbag_avg,globalSHAPImp_oob,globalSHAPImp_inbag, rlvFtrs)
    else:    
        shap_values = shap.TreeExplainer(rf).shap_values(x_train)
        shap_averages = np.mean(np.absolute(shap_values), axis=0) # change to mean

        shap_vals = np.vstack((shap_vals, shap_values))
        shap_avs = np.vstack((shap_avs, shap_averages))
  
  ft_importances = np.delete(ft_importances, (0), axis=0) # Deletes the initialization

  if (inoutbag):
      shap_vals_oob = np.delete(shap_vals_oob, (0), axis=0) # Deletes the initialization
      shap_vals_in = np.delete(shap_vals_in, (0), axis=0)
      shap_avs_oob = np.delete(shap_avs_oob, (0), axis=0) # Deletes the initialization
      shap_avs_in = np.delete(shap_avs_in, (0), axis=0)
      return(shap_vals_oob,shap_vals_in,shap_avs_oob, shap_avs_in, ft_importances, allDFs, n, y_train)
  else: 
      shap_vals = np.delete(shap_vals, (0), axis=0) # Deletes the initialization
      shap_avs = np.delete(shap_avs, (0), axis=0) # Deletes the initialization
      return(shap_vals, shap_avs, ft_importances, allDFs, n)

```
### Python Code: Power Simulation, inoutbag=True
```{python}
# shap_vals_oob,shap_vals_in,shap_avs_oob, shap_avs_in, ft_importances, allDFs, n, y_train = SimulateData_simple(n=120, M=12, inoutbag = True)

print(datetime.datetime.now())
start_time = time.time()

shap_vals_oob,shap_vals_in,shap_avs_oob, shap_avs_in, ft_importances, allDFs, n, y_train = SimulateData_simple(n=120, M=100, inoutbag = True)

print("--- %s seconds ---" , (time.time() - start_time))
y_train = np.concatenate(y_train)

```

```{python}
print(shap_vals_oob.shape,shap_avs_oob.shape, y_train.shape) 
# qi: note: (n_samples*n_simulation,Nr.features)  120*100 = (12000,5)
# qi: note: (1*n_simulation,Nr.features)  1*100 = (100,5)

shap_oob_all_wghted = (shap_vals_oob.T*y_train).T
shap_inbag_all_wghted = (shap_vals_in.T*y_train).T

shap_vals_oob.tofile("data/shap_vals_oob_sim1_power.tsv","\t")
shap_vals_in.tofile("data/shap_vals_in_sim1_power.tsv","\t")
y_train.tofile("data/y_train_sim1_power.tsv","\t")

shap_avs_oob.tofile("data/shap_avs_sim1_oob_power.tsv","\t")
shap_avs_in.tofile("data/shap_avs_sim1_inbag_power.tsv","\t")

print(shap_vals_oob.shape, shap_vals_in.shape, y_train.shape)
print(shap_inbag_all_wghted.shape,shap_oob_all_wghted.shape)
```
### R Code: copy simulated data value to R enviroment
```{r}
shap_vals_oob=py$shap_vals_oob
shap_vals_in=py$shap_vals_in
shap_avs_oob=py$shap_avs_oob
shap_avs_in=py$shap_avs_in
ft_importances= py$ft_importances
allDFs=py$allDFs
n=py$n
y_train=py$y_train
```

```{r,fig.width=WIDTH*2,fig.height=HEIGHT}
# qi: question: should we set the y axis to be the same scale for comparison reason?

par(mfrow=c(1,2),oma = c(0, 0, 2, 0))
boxplot(shap_avs_oob,main = 'OOB')
boxplot(shap_avs_in, main = 'Inbag')

mtext('Power Simulation, inoutbag = True', outer = TRUE, cex = 1.5)

```
### Power Simulation, inoutbag=False
```{python}
shap_vals, shap_avs, ft_importances, allDFs, n = SimulateData_simple(n=120, M=100)
shap_avs.tofile("shap_avs_sim1_power.tsv","\t")

def beeswarm(simulation, n,shap_=shap_vals): # plot the shap plot of the Mth simulation 
    import shap 
    shap.summary_plot(shap_[0+n*simulation:n+n*simulation], allDFs[simulation], show=True)

M = 0 
beeswarm(M, n) # plots the M-th simulation 
```

```{r,fig.width=WIDTH*2,fig.height=HEIGHT}
shap_avs = py$shap_avs
ft_importances = py$ft_importances
  
par(mfrow=c(1,2),oma = c(0, 0, 2, 0))
boxplot(shap_avs,main = 'shap_avs')
boxplot(ft_importances,main = 'ft_importances')
mtext('Power Simulation, inoutbag = False', outer = TRUE, cex = 1.5)

```

```{r,echo=FALSE}
save(shap_avs_oob, shap_avs_in, shap_avs, file = "SHAP_bias/data/Simulation1_power.rda")
```


### Null Simulation,inoutbag=False
```{python}
start_time = time.time()

shap_vals0, shap_avs0, ft_importances0, allDFs0, n0 = SimulateData_simple(n=120, M=100, relevance=0)

print("--- %s seconds ---" % (time.time() - start_time))
```

```{r,fig.width=WIDTH*2,fig.height=HEIGHT}
shap_avs0 = py$shap_avs0
ft_importances0 = py$ft_importances0

par(mfrow=c(1,2),oma = c(0, 0, 2, 0))
boxplot(shap_avs0,main='shap_avs0')
boxplot(ft_importances0,main = 'ft_importances0')
mtext('Null Simulation, inoutbag = False', outer = TRUE, cex = 1.5)
```


### Null Simulation, inoutbag=True
```{python}
start_time = time.time()
print(datetime.datetime.now())
shap_vals_oob0,shap_vals_in0,shap_avs_oob0, shap_avs_in0, ft_importances0, allDFs0, n0, y_train0 = SimulateData_simple(n=120, M=100, relevance=0, inoutbag = True)
y_train0 = np.concatenate(y_train0)

# save data 
shap_vals_oob0.tofile("data/shap_vals_oob_sim1_null.tsv","\t")
shap_vals_in0.tofile("data/shap_vals_in_sim1_null.tsv","\t")
y_train0.tofile("data/y_train_sim1_null.tsv","\t")
shap_avs_oob0.tofile("shap_avs_sim1_oob_null.tsv","\t")
shap_avs_in0.tofile("shap_avs_sim1_inbag_null.tsv","\t")

print("--- %s seconds ---" , (time.time() - start_time))
print(shap_avs_oob0.shape)

```


```{r,fig.width=WIDTH*2,fig.height=HEIGHT}
shap_avs_oob0 = py$shap_avs_oob0
shap_avs_in0  = py$shap_avs_in0

par(mfrow=c(1,2),oma = c(0, 0, 2, 0))
boxplot(shap_avs_oob0,main='shap_avs_oob0')
boxplot(shap_avs_in0,main = 'shap_avs_in0')
mtext('Null Simulation, inoutbag = True', outer = TRUE, cex = 1.5)
```


```{r,echo=FALSE}
save(shap_avs_oob0, shap_avs_in0, shap_avs0, file = "SHAP_bias/data/Simulation1_null.rda")
```

```{python}
# !tar chvfz SHAPsimData.tar.gz data/*
```


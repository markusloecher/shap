# Honest Trees {#Honest_Trees}

Inspired by "honest trees": we want to recompute SHAP values on trees where the node predictions (based on the inbag data) have been replaced by oob estimates. 
(Problem: the oob data are much smaller and lead to many empty nodes, so we have to prune the trees!)

```{r, echo=FALSE}
library(ggplot2)

load("data/shap_SHAP_TitanicRegV3.rda")
 #shap$g= apply(shap[,c("bag", "feature")],1,paste0,collapse="-")
 shap = subset(shap, bag!="oob3")
# shap2 = split(shap$shap, g)
# shap_scaled = unsplit(lapply(shap2,scale), g)

 shap_scaled = shap
 for (g in unique(shap$bag)) {
   jj = (g == shap$bag)
   shap_scaled[jj,1] = scale(shap[jj,1],FALSE, quantile(shap[jj,1],0.8))# IQR(shap[jj,1]))
 }
```

In analogy to Figure \@ref(fig:shap-titanic-inbag-oob-boxplots-fig) we use boxplots facilitate easy comparison between inbag and oob as well as honest tree versions ("ob2") SHAP values


```{r shap-honest-boxplots, fig.cap='The difference in SHAP distributions the sign of $Y_{train}$ is greatest for informative features. ', fig.width=8, echo=FALSE}
ggplot(shap_scaled, aes(x=Y_train2, y=shap, fill=bag)) + geom_boxplot() + facet_wrap(~ feature)
```


In analogy to Figure \@ref(fig:inbag-oob-correlations-fig)


```{r, fig.width = 7, fig.height = 6}
par(mfrow=c(2,2))
for (j in 1:4){
  fit = lm(shap_oob_avg[,j] ~ shap_inbag_avg[,j])
  R2 = round(summary(fit)$r.sq,2)
  
  plot(shap_inbag_avg[,j],shap_oob_avg[,j],col=rgb(0,0,1,0.5), pch=20,cex=0.75, xlab = "Inbag SHAP", ylab="oob shap", main = paste0(colnames(shap_inbag_avg)[j], " (R2 =", R2,")"));grid()
  
  abline(fit,col=2)
}
```

```{r, fig.width = 7, fig.height = 6}
par(mfrow=c(2,2))
for (j in 1:4){
  fit = lm(shap_oob2_avg[,j] ~ shap_inbag_avg[,j])
  R2 = round(summary(fit)$r.sq,2)
  
  plot(shap_inbag_avg[,j],shap_oob2_avg[,j],col=rgb(0,0,1,0.5), pch=20,cex=0.75, xlab = "Inbag SHAP", ylab="oob2 shap", main = paste0(colnames(shap_inbag_avg)[j], " (R2 =", R2,")"));grid()
  
  abline(fit,col=2)
}
```